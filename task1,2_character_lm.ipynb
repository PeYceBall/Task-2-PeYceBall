{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2. Language modeling.\n",
    "\n",
    "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
    "\n",
    "\n",
    "\n",
    "## Task 1. Character-based language modeling: data preparation (15 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (1 points)**\n",
    "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infile(infile):\n",
    "    words = []\n",
    "    with codecs.open(infile, \"r\", \"utf_8_sig\") as f:\n",
    "        for line in f.readlines():\n",
    "            line_words = line.lower().split('\\t')\n",
    "            if len(line_words) == 3 and ' ' not in line_words[1]:\n",
    "                # I decided to include both lemma and form in dataset\n",
    "                words += line_words[:-1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18426 1834 1844\n",
      "валлонский валлонскому незаконченный незаконченным истрёпывать истрёпывав личный личного серьга серьгам\n"
     ]
    }
   ],
   "source": [
    "train_words = read_infile(\"russian-train-high.txt\")\n",
    "dev_words = read_infile(\"russian-dev.txt\")\n",
    "test_words = read_infile(\"russian-test.txt\")\n",
    "print(len(train_words), len(dev_words), len(test_words))\n",
    "print(*train_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "from collections import Iterable\n",
    "\n",
    "class SimpleVocabulary:\n",
    "    def __init__(self):\n",
    "        self.sym_to_ind = {}\n",
    "        self.ind_to_sym = {}\n",
    "    \n",
    "    def fit(self, text):\n",
    "        symbols = set(('<PAD>', '<BEG>', '<END>'))\n",
    "        for word in text:\n",
    "            symbols.update(word)\n",
    "        \n",
    "        self.sym_to_ind = {val: i for i, val in enumerate(symbols)}\n",
    "        self.ind_to_sym = {v: k for k, v in self.sym_to_ind.items()}\n",
    "    \n",
    "    # we imply that each string in batch is exactly one symbol(incl. special)\n",
    "    def __call__(self, batch):\n",
    "        if isinstance(batch, Iterable) and not isinstance(batch, str):\n",
    "            looked_up_batch = [self(sample) for sample in batch]\n",
    "        else:\n",
    "            return self.sym_to_ind[batch]\n",
    "\n",
    "        return looked_up_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sym_to_ind)\n",
    "\n",
    "vocab = SimpleVocabulary()\n",
    "vocab.fit([list(x) for x in train_words])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    \n",
    "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
    "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
    "        for the language model that obtain these word as input.        \n",
    "        \"\"\"\n",
    "        word = ['<BEG>'] + list(self.data[index]) + ['<END>']\n",
    "        source = torch.tensor(self.vocab(word[:-1]))\n",
    "        target = torch.tensor(self.vocab(word[1:]))\n",
    "        return source, target\n",
    "\n",
    "    def __len__(self):        \n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_words, vocab)\n",
    "dev_dataset = Dataset(dev_words, vocab)\n",
    "test_dataset = Dataset(test_words, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 14])\n",
      "torch.Size([1, 12])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 7])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from itertools import islice\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=1)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=1)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "\n",
    "for x in islice(train_data_loader, 10):\n",
    "    print(x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5) 1 point** Explain, why this does not work with larger batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If batch_size > 1 then different words in batch may have different length. This implies their sources and targets may have different shape and thus they cannot be united in one tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, length, dim, pad_symbol):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = length - vec.size(dim)\n",
    "    return torch.cat([vec, pad_symbol \n",
    "                      * torch.ones(*pad_size, dtype=torch.long)], dim=dim)\n",
    "\n",
    "class Padder:\n",
    "    \n",
    "    def __init__(self, dim=0, pad_symbol=0):\n",
    "        self.dim = dim\n",
    "        self.pad_symbol = pad_symbol\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # batch shape: (batch_size, 2, word_len)\n",
    "        # find longest sequence\n",
    "        max_len = max(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # pad according to max_len\n",
    "        batch = list(map(lambda x: \n",
    "                     (pad_tensor(x[0], max_len, self.dim, self.pad_symbol),\n",
    "                     pad_tensor(x[1], max_len, self.dim, self.pad_symbol)), \n",
    "                    batch))\n",
    "        # stack all\n",
    "        xs = torch.stack(list(map(lambda x: x[0], batch)), dim=0)\n",
    "        ys = torch.stack(list(map(lambda x: x[1], batch)), dim=0)\n",
    "\n",
    "        return xs, ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 20])\n",
      "torch.Size([1024, 28])\n",
      "torch.Size([1024, 21])\n",
      "torch.Size([1024, 22])\n",
      "torch.Size([1024, 21])\n",
      "torch.Size([1024, 26])\n",
      "torch.Size([1024, 21])\n",
      "torch.Size([1024, 23])\n",
      "torch.Size([1024, 22])\n",
      "torch.Size([1024, 21])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "padder = Padder(dim=0, pad_symbol=vocab.sym_to_ind['<PAD>'])\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                               batch_size=1024, collate_fn=padder)\n",
    "dev_data_loader = DataLoader(dev_dataset,\n",
    "                               batch_size=1024, collate_fn=padder)\n",
    "test_data_loader = DataLoader(test_dataset,\n",
    "                               batch_size=1024, collate_fn=padder)\n",
    "\n",
    "\n",
    "for x in islice(train_data_loader, 10):\n",
    "    print(x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Character-based language modeling. (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
    "1. **Embedding** layer that transforms input symbols into vectors.\n",
    "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
    "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_size):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.gru = nn.GRU(embeddings_dim, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "                \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        emb = self.emb(inputs)\n",
    "        # save last hidden_state\n",
    "        gru, self.last_hidden = self.gru(emb, hidden)\n",
    "\n",
    "        lin = self.linear(gru)\n",
    "        return lin\n",
    "        # softmax is done in CrossEntropyLoss\n",
    "#         return self.softmax(lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_batch(model, criterion, x, y):\n",
    "    y_pred = model(x)\n",
    "    vocab_size = y_pred.shape[2]\n",
    "    return criterion(y_pred.reshape((-1, vocab_size)), y.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, criterion, x, y, optimizer):\n",
    "    loss = validate_on_batch(model, criterion, x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
    "\n",
    "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 30\n",
    "hidden_size = 50 \n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "padder = Padder(dim=0, pad_symbol=vocab.sym_to_ind['<PAD>'])\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                               batch_size=batch_size, collate_fn=padder)\n",
    "model = RNNLM(len(vocab), embeddings_dim, hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab.sym_to_ind['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch in train_data_loader:\n",
    "        loss = train_on_batch(model, criterion, batch[0], batch[1], optimizer)\n",
    "        epoch_loss.append(float(loss))\n",
    "        \n",
    "    # save average loss of each epoch        \n",
    "    loss_history.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(model, batch):\n",
    "    with torch.no_grad():\n",
    "        pred = nn.functional.softmax(model(batch), dim=2)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = DataLoader(test_dataset,\n",
    "                               batch_size=1, collate_fn=padder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1824 word: общеобразовательный<END>\n",
      "('о', 0.071) ('б', 0.267) ('щ', 0.032) ('е', 0.382) ('о', 0.002) ('б', 0.105) ('р', 0.518) ('а', 0.398) ('з', 0.045) ('о', 0.015) ('в', 0.451) ('а', 0.69) ('т', 0.59) ('е', 0.04) ('л', 0.951) ('ь', 0.913) ('н', 0.921) ('ы', 0.599) ('й', 0.721) ('<END>', 0.999)\n",
      "1825 word: общеобразовательного<END>\n",
      "('о', 0.071) ('б', 0.267) ('щ', 0.032) ('е', 0.382) ('о', 0.002) ('б', 0.105) ('р', 0.518) ('а', 0.398) ('з', 0.045) ('о', 0.015) ('в', 0.451) ('а', 0.69) ('т', 0.59) ('е', 0.04) ('л', 0.951) ('ь', 0.913) ('н', 0.921) ('о', 0.262) ('г', 0.296) ('о', 0.981) ('<END>', 0.98)\n",
      "1826 word: фригидный<END>\n",
      "('ф', 0.01) ('р', 0.13) ('и', 0.062) ('г', 0.09) ('и', 0.086) ('д', 0.001) ('н', 0.532) ('ы', 0.672) ('й', 0.738) ('<END>', 1.0)\n",
      "1827 word: фригидной<END>\n",
      "('ф', 0.01) ('р', 0.13) ('и', 0.062) ('г', 0.09) ('и', 0.086) ('д', 0.001) ('н', 0.532) ('о', 0.293) ('й', 0.333) ('<END>', 0.999)\n",
      "1828 word: безмолвный<END>\n",
      "('б', 0.04) ('е', 0.28) ('з', 0.356) ('м', 0.052) ('о', 0.546) ('л', 0.141) ('в', 0.004) ('н', 0.05) ('ы', 0.466) ('й', 0.705) ('<END>', 1.0)\n",
      "1829 word: безмолвный<END>\n",
      "('б', 0.04) ('е', 0.28) ('з', 0.356) ('м', 0.052) ('о', 0.546) ('л', 0.141) ('в', 0.004) ('н', 0.05) ('ы', 0.466) ('й', 0.705) ('<END>', 1.0)\n",
      "1830 word: многолетний<END>\n",
      "('м', 0.039) ('н', 0.059) ('о', 0.741) ('г', 0.572) ('о', 0.903) ('л', 0.151) ('е', 0.075) ('т', 0.25) ('н', 0.001) ('и', 0.695) ('й', 0.033) ('<END>', 0.935)\n",
      "1831 word: многолетним<END>\n",
      "('м', 0.039) ('н', 0.059) ('о', 0.741) ('г', 0.572) ('о', 0.903) ('л', 0.151) ('е', 0.075) ('т', 0.25) ('н', 0.001) ('и', 0.695) ('м', 0.008) ('<END>', 0.544)\n",
      "1832 word: оттопырить<END>\n",
      "('о', 0.071) ('т', 0.233) ('т', 0.019) ('о', 0.103) ('п', 0.133) ('ы', 0.005) ('р', 0.002) ('и', 0.066) ('т', 0.696) ('ь', 0.956) ('<END>', 0.635)\n",
      "1833 word: оттопырьте<END>\n",
      "('о', 0.071) ('т', 0.233) ('т', 0.019) ('о', 0.103) ('п', 0.133) ('ы', 0.005) ('р', 0.002) ('ь', 0.012) ('т', 0.182) ('е', 0.278) ('<END>', 0.078)\n",
      "1834 word: долбануть<END>\n",
      "('д', 0.04) ('о', 0.356) ('л', 0.115) ('б', 0.054) ('а', 0.157) ('н', 0.141) ('у', 0.028) ('т', 0.606) ('ь', 0.84) ('<END>', 0.649)\n",
      "1835 word: долбануть<END>\n",
      "('д', 0.04) ('о', 0.356) ('л', 0.115) ('б', 0.054) ('а', 0.157) ('н', 0.141) ('у', 0.028) ('т', 0.606) ('ь', 0.84) ('<END>', 0.649)\n",
      "1836 word: синеватый<END>\n",
      "('с', 0.094) ('и', 0.049) ('н', 0.335) ('е', 0.251) ('в', 0.063) ('а', 0.177) ('т', 0.424) ('ы', 0.047) ('й', 0.5) ('<END>', 0.999)\n",
      "1837 word: синеватые<END>\n",
      "('с', 0.094) ('и', 0.049) ('н', 0.335) ('е', 0.251) ('в', 0.063) ('а', 0.177) ('т', 0.424) ('ы', 0.047) ('е', 0.014) ('<END>', 0.998)\n",
      "1838 word: колониальный<END>\n",
      "('к', 0.063) ('о', 0.364) ('л', 0.192) ('о', 0.272) ('н', 0.156) ('и', 0.444) ('а', 0.0) ('л', 0.2) ('ь', 0.922) ('н', 0.801) ('ы', 0.483) ('й', 0.811) ('<END>', 1.0)\n",
      "1839 word: колониальному<END>\n",
      "('к', 0.063) ('о', 0.364) ('л', 0.192) ('о', 0.272) ('н', 0.156) ('и', 0.444) ('а', 0.0) ('л', 0.2) ('ь', 0.922) ('н', 0.801) ('о', 0.278) ('м', 0.147) ('у', 0.462) ('<END>', 0.98)\n",
      "1840 word: надавливать<END>\n",
      "('н', 0.068) ('а', 0.388) ('д', 0.109) ('а', 0.22) ('в', 0.331) ('л', 0.195) ('и', 0.604) ('в', 0.768) ('а', 0.941) ('т', 0.614) ('ь', 0.982) ('<END>', 0.847)\n",
      "1841 word: надавливало<END>\n",
      "('н', 0.068) ('а', 0.388) ('д', 0.109) ('а', 0.22) ('в', 0.331) ('л', 0.195) ('и', 0.604) ('в', 0.768) ('а', 0.941) ('л', 0.134) ('о', 0.261) ('<END>', 0.729)\n",
      "1842 word: истерический<END>\n",
      "('и', 0.029) ('с', 0.231) ('т', 0.144) ('е', 0.087) ('р', 0.36) ('и', 0.398) ('ч', 0.02) ('е', 0.772) ('с', 0.871) ('к', 0.973) ('и', 0.708) ('й', 0.804) ('<END>', 1.0)\n",
      "1843 word: истерический<END>\n",
      "('и', 0.029) ('с', 0.231) ('т', 0.144) ('е', 0.087) ('р', 0.36) ('и', 0.398) ('ч', 0.02) ('е', 0.772) ('с', 0.871) ('к', 0.973) ('и', 0.708) ('й', 0.804) ('<END>', 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_data_loader):\n",
    "   \n",
    "        word = [vocab.ind_to_sym[int(sym)] for sym in batch[1][0]]\n",
    "        probs = predict_on_batch(model, batch[0])\n",
    "        word_probs = [round(float(probs[0, i, batch[1][0, i]]), 3)\n",
    "                      for i in range(len(batch[1][0]))]\n",
    "        \n",
    "        if i > len(test_data_loader) - 21:\n",
    "            print(i, 'word:', ''.join(word))\n",
    "            print(*list(zip(word, word_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that probabilities are pretty high for most words, also `<END>` symbol has very high probability at the end of words, so our language model knows when the word should be finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, max_length=20, start_index=1, end_index=2):\n",
    "    prev_index = start_index\n",
    "    word = []\n",
    "    hidden = None\n",
    "    for _ in range(max_length):\n",
    "        # using batch of one letter\n",
    "        batch = torch.tensor(prev_index).reshape((1, 1))\n",
    "        probs = torch.nn.functional.softmax(model(batch, hidden), dim=2)\n",
    "        new_index = int(torch.multinomial(probs.reshape(-1), 1))\n",
    "        word.append(new_index)\n",
    "        if new_index == end_index:\n",
    "            break\n",
    "            \n",
    "        prev_index = new_index\n",
    "        hidden = model.last_hidden\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хригейцок<END>\n",
      "блатятетька<END>\n",
      "потадажка<END>\n",
      "строесь<END>\n",
      "перефермуав<END>\n",
      "ответника<END>\n",
      "мочажка<END>\n",
      "двтождах<END>\n",
      "изучитемия<END>\n",
      "непнитарованный<END>\n",
      "выматечны<END>\n",
      "ладие<END>\n",
      "алкнивых<END>\n",
      "опода<END>\n",
      "мокатять<END>\n",
      "всхажтный<END>\n",
      "дамляю<END>\n",
      "свойственного<END>\n",
      "проболками<END>\n",
      "изнойный<END>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q\\Anaconda2\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    word = generate(model, start_index=vocab.sym_to_ind['<BEG>'],\n",
    "                    end_index=vocab.sym_to_ind['<END>'])\n",
    "    word = ''.join([vocab.ind_to_sym[ind] for ind in word])\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbol `<END>` was left in word to indicate that language model chose to finish the word on its own. We can see that the words look pretty similar to real russian words, we even got one real word -- \"свойственного\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(model, batch_size, max_length = 20, start_index=1,\n",
    "                   end_index=2):\n",
    "    prev_index = start_index * torch.ones((batch_size, 1), dtype=torch.long)\n",
    "    # pad small words\n",
    "    words = vocab.sym_to_ind['<PAD>'] * torch.ones(\n",
    "        (batch_size, max_length), dtype=torch.long\n",
    "    ) \n",
    "    \n",
    "    finished_mask = torch.zeros(batch_size, dtype=torch.bool)\n",
    "    hidden = None\n",
    "    for i in range(max_length):\n",
    "        probs = torch.nn.functional.softmax(\n",
    "            model(prev_index, hidden), dim=2\n",
    "        )\n",
    "        new_index = torch.multinomial(probs.reshape((batch_size, -1)), 1)\n",
    "        # adding only to unfinished words\n",
    "        append_ind = ~finished_mask\n",
    "        words[append_ind, i] = new_index.reshape(-1)[append_ind]\n",
    "        #update mask\n",
    "        finished_mask = finished_mask | (new_index.reshape(-1) == end_index)\n",
    "        if finished_mask.all():\n",
    "            break\n",
    "            \n",
    "        prev_index = new_index\n",
    "        hidden = model.last_hidden\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "заморальникой<END>\n",
      "рвкивает<END>\n",
      "задальных<END>\n",
      "нотранских<END>\n",
      "гахстёрный<END>\n",
      "лесорогазный<END>\n",
      "антутка<END>\n",
      "подпостождённом<END>\n",
      "сондеми<END>\n",
      "социотить<END>\n",
      "двоежу<END>\n",
      "исмитаительский<END>\n",
      "чревого<END>\n",
      "внедженный<END>\n",
      "необ<END>\n",
      "выдаем<END>\n",
      "отчарвье<END>\n",
      "склозном<END>\n",
      "паскать<END>\n",
      "непрепроскивать<END>\n"
     ]
    }
   ],
   "source": [
    "generated = []\n",
    "for _ in range(2):\n",
    "    generated += generate_batch(model, batch_size=10,\n",
    "                                start_index=vocab.sym_to_ind['<BEG>'],\n",
    "                                end_index=vocab.sym_to_ind['<END>'])\n",
    "\n",
    "transformed = [[vocab.ind_to_sym[int(ind)] \n",
    "                for ind in word if ind != vocab.sym_to_ind['<PAD>']]\n",
    "               for word in generated]\n",
    "\n",
    "for elem in transformed:\n",
    "    print(\"\".join(elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLM_experiment(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_size, gru_kwargs={}):\n",
    "        super(RNNLM_experiment, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.gru = nn.GRU(embeddings_dim, hidden_size, batch_first=True,\n",
    "                         **gru_kwargs)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.gru_kwargs = gru_kwargs\n",
    "                \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        emb = self.emb(inputs)\n",
    "        # save last hidden_state\n",
    "        gru, self.last_hidden = self.gru(emb, hidden)\n",
    "\n",
    "        lin = self.linear(gru)\n",
    "        return lin\n",
    "        # softmax is done in CrossEntropyLoss\n",
    "#         return self.softmax(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 30\n",
    "hidden_size = 50 \n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "gru_kwargs = {\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout\n",
    "}\n",
    "\n",
    "padder = Padder(dim=0, pad_symbol=vocab.sym_to_ind['<PAD>'])\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                               batch_size=batch_size, collate_fn=padder)\n",
    "model_exp = RNNLM_experiment(len(vocab), embeddings_dim, hidden_size,\n",
    "                             gru_kwargs)\n",
    "optimizer = torch.optim.Adam(model_exp.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab.sym_to_ind['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch in train_data_loader:\n",
    "        loss = train_on_batch(model_exp, criterion, batch[0], batch[1],\n",
    "                              optimizer)\n",
    "        epoch_loss.append(float(loss))\n",
    "        \n",
    "    # save average loss of each epoch        \n",
    "    loss_history.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "намовомь<END>\n",
      "объад<END>\n",
      "хос<END>\n",
      "ликикари<END>\n",
      "обестравывать<END>\n",
      "обеничитель<END>\n",
      "хонковданный<END>\n",
      "лесной<END>\n",
      "плоногинты<END>\n",
      "сацогись<END>\n",
      "нисяс<END>\n",
      "сульникольный<END>\n",
      "плотромных<END>\n",
      "верагис<END>\n",
      "провотерев<END>\n",
      "аоголивайтесь<END>\n",
      "йожрёрном<END>\n",
      "свлодим<END>\n",
      "дисера<END>\n",
      "прикнуть<END>\n"
     ]
    }
   ],
   "source": [
    "generated = []\n",
    "for _ in range(2):\n",
    "    generated += generate_batch(model_exp, batch_size=10,\n",
    "                                start_index=vocab.sym_to_ind['<BEG>'],\n",
    "                                end_index=vocab.sym_to_ind['<END>'])\n",
    "\n",
    "transformed = [[vocab.ind_to_sym[int(ind)] \n",
    "                for ind in word if ind != vocab.sym_to_ind['<PAD>']]\n",
    "               for word in generated]\n",
    "\n",
    "for elem in transformed:\n",
    "    print(\"\".join(elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that model is underfitted, because of increased perplexity(it has now more parameters). Maybe we should train it more and decrease number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 30\n",
    "hidden_size = 50 \n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "gru_kwargs = {\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout\n",
    "}\n",
    "\n",
    "padder = Padder(dim=0, pad_symbol=vocab.sym_to_ind['<PAD>'])\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                               batch_size=batch_size, collate_fn=padder)\n",
    "model_exp = RNNLM_experiment(len(vocab), embeddings_dim, hidden_size,\n",
    "                             gru_kwargs)\n",
    "optimizer = torch.optim.Adam(model_exp.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab.sym_to_ind['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch in train_data_loader:\n",
    "        loss = train_on_batch(model_exp, criterion, batch[0], batch[1],\n",
    "                              optimizer)\n",
    "        epoch_loss.append(float(loss))\n",
    "        \n",
    "    # save average loss of each epoch        \n",
    "    loss_history.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "посведил<END>\n",
      "каяма<END>\n",
      "ковит<END>\n",
      "суммунен<END>\n",
      "пподстлельный<END>\n",
      "осными<END>\n",
      "высчитать<END>\n",
      "обленеческимо<END>\n",
      "анфонка<END>\n",
      "кликоходный<END>\n",
      "окенный<END>\n",
      "располител<END>\n",
      "безрабомный<END>\n",
      "кала<END>\n",
      "отквожелывать<END>\n",
      "довадать<END>\n",
      "стрятось<END>\n",
      "преплескат<END>\n",
      "озтоженный<END>\n",
      "голбочном<END>\n"
     ]
    }
   ],
   "source": [
    "generated = []\n",
    "for _ in range(2):\n",
    "    generated += generate_batch(model_exp, batch_size=10,\n",
    "                                start_index=vocab.sym_to_ind['<BEG>'],\n",
    "                                end_index=vocab.sym_to_ind['<END>'])\n",
    "\n",
    "transformed = [[vocab.ind_to_sym[int(ind)] \n",
    "                for ind in word if ind != vocab.sym_to_ind['<PAD>']]\n",
    "               for word in generated]\n",
    "\n",
    "for elem in transformed:\n",
    "    print(\"\".join(elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the words are better, we even get some real ones. Probably with more data and more layers we could achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
